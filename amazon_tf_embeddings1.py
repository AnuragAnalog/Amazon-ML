# -*- coding: utf-8 -*-
"""Amazon TF Embeddings1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XIvBlkksvCa-wYfTRhbGXD5vm4q2dEk-

# Amazon TF Embeddings
"""

# Download the metadata

!wget -O "metadata.zip" "https://he-s3.s3.ap-southeast-1.amazonaws.com/media/hackathon/amazon-ml-challenge/product-browse-node-classification-2-7ff04e5a/546b594ee0a211eb.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=54dae659cc43a005978d2bb47972792416a7a409b4062ad846bebf3face25aaf&X-Amz-Date=20210731T052448Z&X-Amz-Credential=AKIA6I2ISGOYH7WWS3G5%2F20210731%2Fap-southeast-1%2Fs3%2Faws4_request"

# Required modules

import os
import csv
import numpy as np
import pandas as pd
import tensorflow_hub as hub

from tqdm import tqdm
from zipfile import ZipFile
from sklearn.linear_model import SGDClassifier, LogisticRegression, PassiveAggressiveClassifier

# Extracting the metadata

with ZipFile('metadata.zip', 'r') as zf:
    zf.extractall("./")

# Get dataset link

link = open('datasetLink.txt').read()

# Download Actual Dataset

os.system(f"wget -O 'dataset.zip' {link}")

# Extracting the dataset

with ZipFile('dataset.zip', 'r') as zf:
    zf.extractall('./')

# Some paths

main_dir = './dataset'
train_dir = os.path.join(main_dir, 'train.csv')
test_dir = os.path.join(main_dir, 'test.csv')

# Load the train dataset

train_embeddings = list()

embed = hub.load("https://tfhub.dev/google/nnlm-en-dim128/2")

train_chunks = pd.read_csv(train_dir, escapechar="\\", quoting=csv.QUOTE_NONE, chunksize=10000)
for df in tqdm(train_chunks):
    df = df.replace(np.nan, "")
    df['text'] = df['BRAND'] + df['TITLE'] + df['DESCRIPTION'] + df['BULLET_POINTS']
    del df['BRAND'], df['TITLE'], df['DESCRIPTION'], df['BULLET_POINTS']
    train_embed = embed(df['text'])
    train_embeddings.append(train_embed)

train = np.concatenate(train_embeddings)

# Train labels

train_labels = pd.read_csv(train_dir, escapechar="\\", quoting=csv.QUOTE_NONE, usecols=['BROWSE_NODE_ID']).values

mini_batch = 10000
classes = np.unique(train_labels)

sgd = PassiveAggressiveClassifier()
for first in tqdm(range(0, train.shape[0], mini_batch)):
    X_batch = train[first:first+mini_batch, :]
    y_batch = train_labels[first:first+mini_batch]
    sgd.partial_fit(X_batch, y_batch, classes=classes)

# Load the test dataset

test_embeddings = list()

embed = hub.load("https://tfhub.dev/google/nnlm-en-dim128/2")

test_chunks = pd.read_csv(test_dir, escapechar="\\", quoting=csv.QUOTE_NONE, chunksize=10000)
for df in tqdm(test_chunks):
    df = df.replace(np.nan, "")
    df['text'] = df['BRAND'] + df['TITLE'] + df['DESCRIPTION'] + df['BULLET_POINTS']
    del df['BRAND'], df['TITLE'], df['DESCRIPTION'], df['BULLET_POINTS']
    test_embed = embed(df['text'])
    test_embeddings.append(test_embed)

test = np.concatenate(test_embeddings)

# Train labels

test_id = pd.read_csv(test_dir, escapechar="\\", quoting=csv.QUOTE_NONE, usecols=['PRODUCT_ID']).values

# Test prediction

test_pred = list()

for first in tqdm(range(0, test.shape[0], mini_batch)):
    X_batch = test[first:first+mini_batch, :]
    pred = sgd.predict(X_batch).tolist()
    test_pred.extend(pred)

# Output

submission = pd.DataFrame()
submission['PRODUCT_ID'] = np.squeeze(test_id)
submission['BROWSE_NODE_ID'] = test_pred
submission.to_csv("output6.csv", index=False)